{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Object Detection",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "authorship_tag": "ABX9TyN9INFdReAh7Z1iZDfRsUhf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nicolasalan/Object-Detection-env/blob/main/Object_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Setup**"
      ],
      "metadata": {
        "id": "bmzWLZXhyCZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHed4LZQSaRb",
        "outputId": "688727d0-b79f-4796-cbfc-6e7ff6f3a80f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed May 11 01:06:48 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Criação de um Dataset Sintético**"
      ],
      "metadata": {
        "id": "8sQJrYK7x_Hq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Clonar repositório**"
      ],
      "metadata": {
        "id": "hXTDoNTA94rq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clonar o repositório:"
      ],
      "metadata": {
        "id": "7fb5tIG0yLw7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2L5aBkxtNmG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26ef8427-0daf-43ee-adee-ca1e0d382393"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'Tensorflow_SynData'...\n",
            "remote: Enumerating objects: 84, done.\u001b[K\n",
            "remote: Counting objects: 100% (67/67), done.\u001b[K\n",
            "remote: Compressing objects: 100% (42/42), done.\u001b[K\n",
            "remote: Total 84 (delta 18), reused 65 (delta 17), pack-reused 17\u001b[K\n",
            "Unpacking objects: 100% (84/84), done.\n"
          ]
        }
      ],
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/fernandomiguel99/Tensorflow_SynData.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Instalação**"
      ],
      "metadata": {
        "id": "6yX47Bt89_Kz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install tensorflow==1.14.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4471_Wvz85d",
        "outputId": "c04d495c-ac3b-4497-e94e-2bd2dfdc5331"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==1.14.0\n",
            "  Downloading tensorflow-1.14.0-cp37-cp37m-manylinux1_x86_64.whl (109.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 109.3 MB 48 kB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.21.6)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.0.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.14.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.44.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.1.0)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "  Downloading tensorboard-1.14.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 77.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.2.0)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "  Using cached tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488 kB)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.8.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.5.3)\n",
            "Collecting keras-applications>=1.0.6\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 9.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (3.17.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14.0) (3.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.3.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (4.2.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.6->tensorflow==1.14.0) (1.5.2)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.0\n",
            "    Uninstalling tensorflow-2.8.0:\n",
            "      Successfully uninstalled tensorflow-2.8.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.14.0 which is incompatible.\u001b[0m\n",
            "Successfully installed keras-applications-1.0.8 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install tensorflow-gpu==1.14.0"
      ],
      "metadata": {
        "id": "MqZbYgoN0C3X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d1c06d21-bfe6-4f1b-a302-25b6b14c63d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-gpu==1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/6d/2348df00a34baaabdef0fdb4f46f962f7a8a6720362c26c3a44a249767ea/tensorflow_gpu-1.14.0-cp27-cp27mu-manylinux1_x86_64.whl (377.0MB)\n",
            "\u001b[K     |████████████████████████████████| 377.0MB 34kB/s \n",
            "\u001b[?25hCollecting grpcio>=1.8.6\n",
            "  Using cached https://files.pythonhosted.org/packages/bd/81/6c704c002a992b9d6466c739e3e7687e0bb2365d8cd63d7fc8e95d502cb6/grpcio-1.41.1.tar.gz\n",
            "Collecting mock>=2.0.0\n",
            "  Using cached https://files.pythonhosted.org/packages/05/d2/f94e68be6b17f46d2c353564da56e6fb89ef09faeeff3313a046cb810ca9/mock-3.0.5-py2.py3-none-any.whl\n",
            "Collecting keras-applications>=1.0.6\n",
            "  Using cached https://files.pythonhosted.org/packages/21/56/4bcec5a8d9503a87e58e814c4e32ac2b32c37c685672c30bc8c54c6e478a/Keras_Applications-1.0.8.tar.gz\n",
            "Collecting wrapt>=1.11.1\n",
            "  Using cached https://files.pythonhosted.org/packages/e6/57/d5673f5201ccbc287e70a574868319267735de3041e496e1e26b48d8f653/wrapt-1.14.1-cp27-cp27mu-manylinux2010_x86_64.whl\n",
            "Collecting protobuf>=3.6.1\n",
            "  Using cached https://files.pythonhosted.org/packages/e9/06/5606088c9fbfb924d0f228350e7ec8707fe8f8a6f3a024ace4f0da81d9ce/protobuf-3.17.3-cp27-cp27mu-manylinux_2_5_x86_64.manylinux1_x86_64.whl\n",
            "Collecting keras-preprocessing>=1.0.5\n",
            "  Using cached https://files.pythonhosted.org/packages/79/4c/7c3275a01e12ef9368a892926ab932b33bb13d55794881e3573482b378a7/Keras_Preprocessing-1.1.2-py2.py3-none-any.whl\n",
            "Collecting gast>=0.2.0\n",
            "  Using cached https://files.pythonhosted.org/packages/48/a3/0bd844c54ae8141642088b7ae09dd38fec2ec7faa9b7d25bb6a23c1f266f/gast-0.5.3.tar.gz\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.14.0) (0.37.1)\n",
            "Collecting numpy<2.0,>=1.14.5\n",
            "  Using cached https://files.pythonhosted.org/packages/3a/5f/47e578b3ae79e2624e205445ab77a1848acdaa2929a00eeef6b16eaaeb20/numpy-1.16.6-cp27-cp27mu-manylinux1_x86_64.whl\n",
            "Collecting termcolor>=1.1.0\n",
            "  Using cached https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
            "Collecting six>=1.10.0\n",
            "  Using cached https://files.pythonhosted.org/packages/d9/5a/e7c31adbe875f2abbb91bd84cf2dc52d792b5a01506781dbcf25c91daf11/six-1.16.0-py2.py3-none-any.whl\n",
            "Collecting absl-py>=0.7.0\n",
            "  Using cached https://files.pythonhosted.org/packages/25/d9/22a0b010487da88200c3f0672c67e892b5399b7a13d105d3863511399f6e/absl-py-0.15.0.tar.gz\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "  Using cached https://files.pythonhosted.org/packages/f4/37/e6a7af1c92c5b68fb427f853b06164b56ea92126bcfd87784334ec5e4d42/tensorboard-1.14.0-py2-none-any.whl\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "  Using cached https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl\n",
            "Collecting google-pasta>=0.1.6\n",
            "  Using cached https://files.pythonhosted.org/packages/59/22/38238cd9b83dd8a857abd7c907b8fe68ceff1611ab3ca5f0e80a5e025956/google_pasta-0.2.0-py2-none-any.whl\n",
            "Collecting backports.weakref>=1.0rc1\n",
            "  Using cached https://files.pythonhosted.org/packages/88/ec/f598b633c3d5ffe267aaada57d961c94fdfa183c5c3ebda2b6d151943db6/backports.weakref-1.0.post1-py2.py3-none-any.whl\n",
            "Collecting enum34>=1.1.6\n",
            "  Using cached https://files.pythonhosted.org/packages/6f/2c/a9386903ece2ea85e9807e0e062174dc26fdce8b05f216d00491be29fad5/enum34-1.1.10-py2-none-any.whl\n",
            "Collecting astor>=0.6.0\n",
            "  Using cached https://files.pythonhosted.org/packages/c3/88/97eef84f48fa04fbd6750e62dcceafba6c63c81b7ac1420856c8dcc0a3f9/astor-0.8.1-py2.py3-none-any.whl\n",
            "Collecting futures>=2.2.0\n",
            "  Using cached https://files.pythonhosted.org/packages/d8/a6/f46ae3f1da0cd4361c344888f59ec2f5785e69c872e175a748ef6071cdb5/futures-3.3.0-py2-none-any.whl\n",
            "Collecting funcsigs>=1; python_version < \"3.3\"\n",
            "  Using cached https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl\n",
            "Collecting h5py\n",
            "  Using cached https://files.pythonhosted.org/packages/12/90/3216b8f6d69905a320352a9ca6802a8e39fdb1cd93133c3d4163db8d5f19/h5py-2.10.0-cp27-cp27mu-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (44.1.1)\n",
            "Collecting werkzeug>=0.11.15\n",
            "  Using cached https://files.pythonhosted.org/packages/cc/94/5f7079a0e00bd6863ef8f1da638721e9da21e5bacee597595b318f71d62e/Werkzeug-1.0.1-py2.py3-none-any.whl\n",
            "Collecting markdown>=2.6.8\n",
            "  Using cached https://files.pythonhosted.org/packages/c0/4e/fd492e91abdc2d2fcb70ef453064d980688762079397f779758e055f6575/Markdown-3.1.1-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: grpcio, keras-applications, gast, termcolor, absl-py\n",
            "  Building wheel for grpcio (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for grpcio: filename=grpcio-1.41.1-cp27-cp27mu-linux_x86_64.whl size=33845663 sha256=044bac075822d5347f5a40506d7c09ce91472485d0813b9a16500b1ff96eec4e\n",
            "  Stored in directory: /root/.cache/pip/wheels/c4/93/65/a16057b837ece5b0b2a2a16253dfa7a649352db2e7370179e8\n",
            "  Building wheel for keras-applications (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-applications: filename=Keras_Applications-1.0.8-cp27-none-any.whl size=50703 sha256=c30465d2550855d592f37ef881e7408089686073b85845950f07c3cadbea9c5c\n",
            "  Stored in directory: /root/.cache/pip/wheels/dd/f2/5d/2689b5547f32c4e258c3b7ccbe7f1d0f2afbb84fb01e830792\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.5.3-cp27-none-any.whl size=19446 sha256=46f65d1962c475b011cec85ba7eb16de7d844ab527c3a1fefafc39b16505e2a0\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/08/d4/3997fab33f1bdf6f98f062863249a607f81d636b403d4d7de1\n",
            "  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for termcolor: filename=termcolor-1.1.0-cp27-none-any.whl size=4833 sha256=cf35d1948d4f7e352e10bae9868e9a0fb5482173647b76bf6df6d6dff5c4775e\n",
            "  Stored in directory: /root/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
            "  Building wheel for absl-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for absl-py: filename=absl_py-0.15.0-cp27-none-any.whl size=132010 sha256=41b13c2bb1422e64e4dafe86e7a0f123f802df567cefdc25208168f96c503553\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/f3/f9/f5b598f0b550f4140026ca6a7aa6b016929cefc369b5104df6\n",
            "Successfully built grpcio keras-applications gast termcolor absl-py\n",
            "Installing collected packages: six, enum34, futures, grpcio, funcsigs, mock, numpy, h5py, keras-applications, wrapt, protobuf, keras-preprocessing, gast, termcolor, absl-py, werkzeug, markdown, tensorboard, tensorflow-estimator, google-pasta, backports.weakref, astor, tensorflow-gpu\n",
            "  Found existing installation: numpy 1.13.3\n",
            "    Uninstalling numpy-1.13.3:\n",
            "      Successfully uninstalled numpy-1.13.3\n",
            "Successfully installed absl-py-0.15.0 astor-0.8.1 backports.weakref-1.0.post1 enum34-1.1.10 funcsigs-1.0.2 futures-3.3.0 gast-0.5.3 google-pasta-0.2.0 grpcio-1.41.1 h5py-2.10.0 keras-applications-1.0.8 keras-preprocessing-1.1.2 markdown-3.1.1 mock-3.0.5 numpy-1.16.6 protobuf-3.17.3 six-1.16.0 tensorboard-1.14.0 tensorflow-estimator-1.14.0 tensorflow-gpu-1.14.0 termcolor-1.1.0 werkzeug-1.0.1 wrapt-1.14.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "astor",
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!pip3 install pyamg\n",
        "!pip3 install opencv-python\n",
        "!pip3 install Pillow\n",
        "!pip3 install pyblur\n",
        "!pip3 install opencv-python"
      ],
      "metadata": {
        "id": "anvOg_PryUqF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fb7dc97-a889-47a1-f460-d5c48e455f3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Requirement already satisfied: pyamg in /usr/local/lib/python3.7/dist-packages (4.2.3)\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from pyamg) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.12.0 in /usr/local/lib/python3.7/dist-packages (from pyamg) (1.4.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python) (1.21.6)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (7.1.2)\n",
            "Requirement already satisfied: pyblur in /usr/local/lib/python3.7/dist-packages (0.2.3)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from pyblur) (0.18.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyblur) (1.4.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from pyblur) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pyblur) (1.21.6)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->pyblur) (2.4.1)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->pyblur) (3.2.2)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image->pyblur) (2021.11.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->pyblur) (1.3.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->pyblur) (2.6.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->pyblur) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->pyblur) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->pyblur) (1.4.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->pyblur) (3.0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image->pyblur) (4.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image->pyblur) (1.15.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Gerar as máscaras**"
      ],
      "metadata": {
        "id": "VPGCPLr9meQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install opencv-python==3.4.2.16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oybp3id4s8uK",
        "outputId": "1411a749-1d21-40c4-c1f5-d9e090b025bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement opencv-python==3.4.2.16 (from versions: 3.4.0.14, 3.4.2.17, 3.4.3.18, 3.4.4.19, 3.4.5.20, 3.4.6.27, 3.4.7.28, 3.4.8.29, 3.4.9.31, 3.4.9.33, 3.4.10.35, 3.4.10.37, 3.4.11.39, 3.4.11.41, 3.4.11.43, 3.4.11.45, 3.4.13.47, 3.4.14.51, 3.4.14.53, 3.4.15.55, 3.4.16.57, 3.4.16.59, 3.4.17.61, 3.4.17.63, 4.0.0.21, 4.0.1.23, 4.0.1.24, 4.1.0.25, 4.1.1.26, 4.1.2.30, 4.2.0.32, 4.2.0.34, 4.3.0.36, 4.3.0.38, 4.4.0.40, 4.4.0.42, 4.4.0.44, 4.4.0.46, 4.5.1.48, 4.5.2.52, 4.5.2.54, 4.5.3.56, 4.5.4.58, 4.5.4.60, 4.5.5.62, 4.5.5.64)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for opencv-python==3.4.2.16\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Tensorflow_SynData/bgsubtractor\n",
        "!python rmbg.py"
      ],
      "metadata": {
        "id": "Jhgv7MEpUxOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Executar através do terminal o seguinte código para verificar se tudo ocorreu corretamente:"
      ],
      "metadata": {
        "id": "b98ZZY6d02Zd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Tensorflow_SynData/synthetic/syndata-generation\n",
        "!python dataset_generator.py --num 5 --dontocclude ./demo_data_dir/objects_dir/ ./output_dir/"
      ],
      "metadata": {
        "id": "uzSqiSTmyXBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para gerar as máscaras das imagens a serem inseridas no SynData Generator dentro da pasta images no caminho Tensorflow_SynData/bgsubtractor/images"
      ],
      "metadata": {
        "id": "sraoOYrn1SxM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dentro da pasta images poderemos encontrar duas outras pastas, a pasta com o nome do objeto, onde estão as imagens e as suas máscaras e a pasta annotations onde estão as anotações das imagens."
      ],
      "metadata": {
        "id": "E8VHcrTm1ZuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Tensorflow_SynData/synthetic/syndata- generation/demo_data_dir/objects_dir/"
      ],
      "metadata": {
        "id": "qcwHFYuh1cV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora o próximo passo é definir os objetos a serem usados, dentro da pasta /demo_data_dir/ existe um arquivo chamado selected.txt, nesse arquivo devemos colocar o nome das pastas dos objetos a serem usados no dataset sintético"
      ],
      "metadata": {
        "id": "m7-Dhyq81mAS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Configurações de arquivo**"
      ],
      "metadata": {
        "id": "SYnouLMQ-PcE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora iremos criar o dataset a partir do comando abaixo:\n"
      ],
      "metadata": {
        "id": "78dKUkGk1u3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Tensorflow_SynData/synthetic/syndata-generation\n",
        "!python dataset_generator.py --num 5 --dontocclude ./demo_data_dir/objects_dir/ ./output_dir/"
      ],
      "metadata": {
        "id": "yyYEvfGb1lfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora que temos o dataset criado, devemos criar as anotações tipo csv que serão usadas. Assim devemos seguir o caminho `Tensorflow_SynData/synthetic/` e executar o script generate_csv da seguinte maneira:\n"
      ],
      "metadata": {
        "id": "rJLe8JAi1zYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python generate_csv.py xml ./syndata- generation/output_dir/annotations synthetic.csv ./syndata- generation/output_dir/images"
      ],
      "metadata": {
        "id": "z-R3faAo1z2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora com arquivo synthetic.csv criado, copiamos ele e colamos na pasta que será criada no seguinte caminho:"
      ],
      "metadata": {
        "id": "NuKp0dpt15LJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Tensorflow_SynData/Scripts/detection_util_scripts/DataSet/` e executamos o script generate_train_eval.py com os seguintes argumentos:"
      ],
      "metadata": {
        "id": "HyxsJJ9m181f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python generate_train_eval_modified.py './DataSet/synthetic.csv' -f 0.8 -o ."
      ],
      "metadata": {
        "id": "4b1YO__N15eJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Tensorflow_SynData/synthetic/syndata- generation/demo_data_dir/objects_dir/` e o arquivo selected.txt(renomear para:Label_Map.txt) e colocar na pasta no caminho `Tensorflow_SynData/Scripts/detection_util_scripts/DataSet/`\n",
        "\n",
        "Gerar o arquivo pbtxt com o script:"
      ],
      "metadata": {
        "id": "InH8xOub2BNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 generate_pbtxt.py txt ./DataSet/Label_Map.txt ./DataSet/Label_Map.pbtxt"
      ],
      "metadata": {
        "id": "W8RbB61h2EmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Com a pasta /DataSet/ pronta, iremos criar os arquivos tfrecord a partir dos comandos abaixo:\n"
      ],
      "metadata": {
        "id": "2YqmNt4F2JiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python generate_tfrecord.py ./DataSet/synthetic_train.csv ./DataSet/Label_Map.pbtxt ./DataSet/images ./DataSet/train.record\n",
        "!python generate_tfrecord.py ./DataSet/synthetic_eval.csv ./DataSet/Label_Map.pbtxt ./DataSet/images ./DataSet/eval.record"
      ],
      "metadata": {
        "id": "MeZvRSwt2J1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Colocar a pasta DataSet com os arquivos recentemente gerados na HOME para ficar assim `home/users/DataSet/`\n",
        "\n",
        "Os arquivos de modelos estarão no caminho `home/fernando/models/`\n",
        "\n",
        "Dentro da pasta do modelo encontraremos alguns arquivo:\n",
        "- frozen_interference_graph.pb -> peso atual\n",
        "- checkpoint um tipo de \"save\" do estado do treinamento, caso você pare e queira recomeçar\n",
        "- pipeline.config é o arquivo que define as variáveis do treinamento do modelo\n",
        "\n",
        "Agora iremos configurar o arquivo pipeline.config(nesse caso do ssd_mobilenet_v2_coco) do modelo escolhido para treinar:\n",
        "- linha 3: num_classes: (número de classes)\n",
        "- No fim do arquivo: verificar no espaços eval_input_reader e train_input_reader se os caminhos estão certos\n",
        "- linha 176: shuffle:true\n",
        "- linha 160: num_steps: 200000\n"
      ],
      "metadata": {
        "id": "jxYsuxjO493Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Treinamento da rede neural**"
      ],
      "metadata": {
        "id": "5QuoeC9x6pvy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para iniciar o treinamento devemos criar um arquivo com o seguinte script:"
      ],
      "metadata": {
        "id": "BHMobPch6uQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PIPELINE_CONFIG_PATH=\"/home/robofeiathome/data/trained_models/lar c2019/pipeline.config\"\n",
        "MODEL_DIR=\"/home/robofeiathome/data/trained_models/larc2019/checkpoint\"\n",
        "NUM_TRAIN_STEPS=200000 \n",
        "SAMPLE_1_OF_N_EVAL_EXAMPLES=10\n",
        "!python2 object_detection/model_main.py -- pipeline_config_path=${PIPELINE_CONFIG_PATH} -- model_dir=${MODEL_DIR} –num_train_steps=${NUM_TRAIN_STEPS} -- sample_1_of_n_eval_examples=${SAMPLE_1_OF_N_EVAL_EXAMPLES} – alsologtostderr"
      ],
      "metadata": {
        "id": "QfqEKI_i4__w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para visualizar o treinamento devemos criar um outro arquivo com o seguinte script:\n"
      ],
      "metadata": {
        "id": "cYEDIM2851E5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_DIR=\"/home/robofeiathome/data/trained_models/larc2019/checkpoint\"\n",
        "!tensorboard --logdir=${MODEL_DIR}"
      ],
      "metadata": {
        "id": "LVimlsdV51lw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para executar o treinamento devemos criar um arquivo de script com o seguinte conteúdo:"
      ],
      "metadata": {
        "id": "MTAIzlKJ58vA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_TYPE = image_tensor\n",
        "PIPELINE_CONFIG_PATH = \"/home/robofeiathome/data/trained_models/lar c2019/pipeline.config\"\n",
        "TRAINED_CKPT_PREFIX= \"/home/robofeiathome/data/trained_models/larc 2019/checkpoint/model.ckpt-52802\"\n",
        "EXPORT_DIR= \"/home/robofeiathome/data/trained_models/larc2019/expor ted\"\n",
        "!python2 object_detection/export_inference_graph.py -- input_type=${INPUT_TYPE} -- pipeline_config_path=${PIPELINE_CONFIG_PATH} -- trained_checkpoint_prefix=${TRAINED_CKPT_PREFIX} – output_directory=${EXPORT_DIR}"
      ],
      "metadata": {
        "id": "7EIUryaE5_YF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRAINED_CKPT_PREFIX - PEGAR O ÍNDICE DO CHECKPOINT DO CAMINHO MODEL_DIR ONDE O MODELO ESTÁ SENDO TREINADO. Ex: 52802"
      ],
      "metadata": {
        "id": "3IrYGIEr6Iv0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exportar do modelo**"
      ],
      "metadata": {
        "id": "VwfyBEv261rf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para exportar o modelo criar um arquivo de script com o seguinte conteúdo:"
      ],
      "metadata": {
        "id": "OfnRSLNk6MiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_TYPE = image_tensor\n",
        "PIPELINE_CONFIG_PATH= \"/home/robofeiathome/data/trained_models/lar c2019/pipeline.config\"\n",
        "TRAINED_CKPT_PREFIX= \"/home/robofeiathome/data/trained_models/larc 2019/checkpoint/model.ckpt-52802\"\n",
        "EXPORT_DIR= \"/home/robofeiathome/data/trained_models/larc2019/expor ted\"\n",
        "!python2 object_detection/export_inference_graph.py -- input_type=${INPUT_TYPE} -- pipeline_config_path=${PIPELINE_CONFIG_PATH} -- trained_checkpoint_prefix=${TRAINED_CKPT_PREFIX} – output_directory=${EXPORT_DIR}"
      ],
      "metadata": {
        "id": "ErSC51Tn6JaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRAINED_CKPT_PREFIX - PEGAR O ÍNDICE DO CHECKPOINT DO CAMINHO MODEL_DIR ONDE O MODELO ESTÁ SENDO TREINADO. Ex: 52802"
      ],
      "metadata": {
        "id": "3FdnkU7D6XR3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Com o treinamento terminado devemos ir até o caminho `home/user/dodo_detector-0.6.1/dodo_detector` e colar os arquivos gerados na pasta exported aqui."
      ],
      "metadata": {
        "id": "Q_M-aozH6avN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criar uma pasta chama images e colocar as imagens que você queira detectar, criar uma pasta chamada savedImages e executar o seguinte comando\n"
      ],
      "metadata": {
        "id": "dYzhOcz36eoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 DetecionOnBatch.py"
      ],
      "metadata": {
        "id": "JoU4FLmY6f1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# baixar VPN fei.com.br VPN\n",
        "# pesquisar sobre ssd mobilenet\n",
        "# pesquisar screen terminal"
      ],
      "metadata": {
        "id": "wWwKNjZwYpVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vis.sh"
      ],
      "metadata": {
        "id": "chY1eKTuRLvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**https://fei.edu.br/vpn/**"
      ],
      "metadata": {
        "id": "Af8HpyKggl4u"
      }
    }
  ]
}